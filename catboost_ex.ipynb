{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CREDIT TO PRUTHA ANNADATE, https://www.kaggle.com/code/pruthaannadate/catboost-march-machine-learning-mania-25\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import brier_score_loss, f1_score, accuracy_score, auc, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_dict(path='march-machine-learning-mania-2025/'):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for fname in os.listdir(path):\n",
    "        data_dict[fname.split('.')[0]] = pd.read_csv(path + fname)\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix feature set logic so you dont need to pass both W and L, append at the end\n",
    "# add models iteratively \n",
    "\n",
    "def bracketology(dataset, feature_set=['WTeamID', 'LTeamID'], model_name='catboost'):\n",
    "    \n",
    "    model_directory = {\n",
    "        'catboost': CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=200),\n",
    "        'lr': LogisticRegression()\n",
    "    }\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    X = dataset[feature_set]\n",
    "    y = dataset['result']\n",
    "    \n",
    "    k_strat = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "    folds = k_strat.split(X, y)\n",
    "\n",
    "    auc = []\n",
    "    accuracy = []\n",
    "    brier_score = []\n",
    "\n",
    "    for train, test in folds:\n",
    "        \n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y.values[train], y.values[test]\n",
    "\n",
    "        model = model_directory[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_preds = model.predict(X_test)\n",
    "        y_probs = model.predict_proba(X_test)\n",
    "        auc.append(roc_auc_score(y_test, y_probs[:,1]))\n",
    "        accuracy.append(accuracy_score(y_test, y_preds))\n",
    "        brier_score.append(brier_score_loss(y_test, y_probs[:,1]))\n",
    "    \n",
    "    model.fit(X, y)\n",
    "\n",
    "    result_dict['model_name'] = model_name\n",
    "    result_dict['model'] = model\n",
    "    result_dict['auc'] = np.average(auc)\n",
    "    result_dict['accuracy'] = np.average(accuracy)\n",
    "    result_dict['brier_score'] = np.average(brier_score)\n",
    "    \n",
    "    return result_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_df_dict()\n",
    "results = pd.concat([dfs['MNCAATourneyDetailedResults'], dfs['WNCAATourneyDetailedResults']])\n",
    "teams = pd.concat([dfs['MTeams'], dfs['WTeams']])\n",
    "\n",
    "results_W = results[['Season', 'DayNum', 'WTeamID', 'WScore']]\n",
    "results_L = results[['Season', 'DayNum', 'LTeamID', 'LScore']]\n",
    "stats = results.drop(['Season', 'DayNum', 'WTeamID', 'WScore', 'Season', 'DayNum', 'LTeamID', 'LScore'], axis=1)\n",
    "\n",
    "full_W = pd.merge(left=results_W, right=teams, left_on='WTeamID', right_on='TeamID', how='left')\n",
    "full_W = full_W.rename(columns={'TeamName':'WTeamName', 'FirstD1Season':'WFirstD1Season', 'LastD1Season':'WLastD1Season'}).drop('TeamID', axis=1)\n",
    "full_L = pd.merge(left=results_L, right=teams, left_on='LTeamID', right_on='TeamID', how='left')\n",
    "full_L = full_L.rename(columns={'TeamName':'LTeamName', 'FirstD1Season':'LFirstD1Season', 'LastD1Season':'LLastD1Season'}).drop('TeamID', axis=1).drop(['Season', 'DayNum'],axis=1)\n",
    "\n",
    "df = pd.concat([full_W, full_L, stats.reset_index()],axis=1).drop(['index', 'WLoc'], axis=1)\n",
    "\n",
    "new_df = pd.DataFrame({})\n",
    "WL_cols = np.sort([col for col in df.columns if 'W' in col or 'L' in col]).tolist()[::-1]\n",
    "\n",
    "reversed_WL_cols = [i for i in WL_cols if i[0] == 'L'] + [i for i in WL_cols if i[0] == 'W']\n",
    "other_cols = np.setdiff1d(df.columns, WL_cols)\n",
    "\n",
    "raw_names = [col.strip(\"W\") for col in WL_cols if 'W' in col]\n",
    "WL_pairs = [[f'W{col}', f'L{col}'] for col in raw_names]\n",
    "\n",
    "for pair in WL_pairs:\n",
    "    new_df[pair] = df[pair[::-1]]\n",
    "\n",
    "df['result'] = 1\n",
    "new_df['result'] = 0\n",
    "\n",
    "new_df[other_cols] = df[other_cols]\n",
    "new_df = new_df[df.columns]\n",
    "\n",
    "master_df = pd.concat([df, new_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since we'll have to predict on combinations, we need to aggregate some stats to generate static features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_teams = np.unique(master_df['WTeamID'].tolist() + master_df['LTeamID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6924851\ttotal: 1.67ms\tremaining: 1.67s\n",
      "200:\tlearn: 0.5401447\ttotal: 205ms\tremaining: 816ms\n",
      "400:\tlearn: 0.4512124\ttotal: 368ms\tremaining: 549ms\n",
      "600:\tlearn: 0.3866578\ttotal: 527ms\tremaining: 350ms\n",
      "800:\tlearn: 0.3416352\ttotal: 685ms\tremaining: 170ms\n",
      "999:\tlearn: 0.3024140\ttotal: 844ms\tremaining: 0us\n",
      "0:\tlearn: 0.6924321\ttotal: 812us\tremaining: 812ms\n",
      "200:\tlearn: 0.5345464\ttotal: 159ms\tremaining: 631ms\n",
      "400:\tlearn: 0.4457989\ttotal: 319ms\tremaining: 477ms\n",
      "600:\tlearn: 0.3845183\ttotal: 477ms\tremaining: 317ms\n",
      "800:\tlearn: 0.3350167\ttotal: 634ms\tremaining: 158ms\n",
      "999:\tlearn: 0.2976283\ttotal: 790ms\tremaining: 0us\n",
      "0:\tlearn: 0.6921007\ttotal: 708us\tremaining: 708ms\n",
      "200:\tlearn: 0.5515049\ttotal: 155ms\tremaining: 615ms\n",
      "400:\tlearn: 0.4610932\ttotal: 315ms\tremaining: 470ms\n",
      "600:\tlearn: 0.4005175\ttotal: 471ms\tremaining: 313ms\n",
      "800:\tlearn: 0.3521907\ttotal: 628ms\tremaining: 156ms\n",
      "999:\tlearn: 0.3132305\ttotal: 783ms\tremaining: 0us\n",
      "0:\tlearn: 0.6922426\ttotal: 647us\tremaining: 646ms\n",
      "200:\tlearn: 0.5508886\ttotal: 153ms\tremaining: 608ms\n",
      "400:\tlearn: 0.4651558\ttotal: 312ms\tremaining: 467ms\n",
      "600:\tlearn: 0.4050201\ttotal: 470ms\tremaining: 312ms\n",
      "800:\tlearn: 0.3610853\ttotal: 625ms\tremaining: 155ms\n",
      "999:\tlearn: 0.3252524\ttotal: 784ms\tremaining: 0us\n",
      "0:\tlearn: 0.6916440\ttotal: 944us\tremaining: 944ms\n",
      "200:\tlearn: 0.5591028\ttotal: 191ms\tremaining: 758ms\n",
      "400:\tlearn: 0.4783332\ttotal: 382ms\tremaining: 571ms\n",
      "600:\tlearn: 0.4217551\ttotal: 575ms\tremaining: 382ms\n",
      "800:\tlearn: 0.3779457\ttotal: 765ms\tremaining: 190ms\n",
      "999:\tlearn: 0.3425707\ttotal: 955ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'catboost',\n",
       " 'model': <catboost.core.CatBoostClassifier at 0x349992ad0>,\n",
       " 'auc': 0.6571591389945052,\n",
       " 'accuracy': 0.6166520210896309,\n",
       " 'brier_score': 0.25390726854978035}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = ['WTeamID', 'LTeamID']\n",
    "feats.extend(['NumOT', 'Season'])\n",
    "bracketology(dataset=master_df, feature_set=feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "input_folder = r\"march-machine-learning-mania-2025/\"\n",
    "\n",
    "# Pertinent file names\n",
    "mens_files = [\"MNCAATourneyCompactResults.csv\", \"MRegularSeasonCompactResults.csv\", \"MTeams.csv\"]\n",
    "womens_files = [\"WNCAATourneyCompactResults.csv\", \"WRegularSeasonCompactResults.csv\", \"WTeams.csv\"]\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# Load in csv data and cast them as dataframes\n",
    "def load_csv_files(file_list):\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        key = os.path.splitext(file)[0]\n",
    "        try:\n",
    "            dataframes[key] = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# Load mens and womens data\n",
    "load_csv_files(mens_files)\n",
    "load_csv_files(womens_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([df, df_inv], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Preprocesses them into dataframes \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m men_results \u001b[38;5;241m=\u001b[39m preprocess_results(\u001b[43mdataframes\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNCAATourneyCompactResults\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m women_results \u001b[38;5;241m=\u001b[39m preprocess_results(dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWNCAATourneyCompactResults\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Merge men's and women's datasets\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# all_results = pd.concat([men_results, women_results], ignore_index=True)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess - the author uses a very minimal feature set so it is very easy to follow\n",
    "def preprocess_results(df):\n",
    "    df = df[['Season', 'WTeamID', 'LTeamID']].copy()\n",
    "    df.loc[:, 'Result'] = 1  # Win label\n",
    "    df_inv = df.copy()\n",
    "    df_inv[['WTeamID', 'LTeamID']] = df[['LTeamID', 'WTeamID']].values\n",
    "    df_inv.loc[:, 'Result'] = 0  # Loss label\n",
    "    return pd.concat([df, df_inv], ignore_index=True)\n",
    "\n",
    "# Preprocesses them into dataframes \n",
    "men_results = preprocess_results(dataframes['MNCAATourneyCompactResults'])\n",
    "women_results = preprocess_results(dataframes['WNCAATourneyCompactResults'])\n",
    "\n",
    "# Merge men's and women's datasets\n",
    "# all_results = pd.concat([men_results, women_results], ignore_index=True)\n",
    "all_results = master_df\n",
    "# Train model\n",
    "X = all_results[['Season', 'WTeamID', 'LTeamID']]\n",
    "y = all_results['Result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6920418\ttotal: 2.55ms\tremaining: 2.55s\n",
      "200:\tlearn: 0.5508519\ttotal: 163ms\tremaining: 649ms\n",
      "400:\tlearn: 0.4606697\ttotal: 320ms\tremaining: 479ms\n",
      "600:\tlearn: 0.3985509\ttotal: 477ms\tremaining: 317ms\n",
      "800:\tlearn: 0.3513631\ttotal: 634ms\tremaining: 157ms\n",
      "999:\tlearn: 0.3133479\ttotal: 791ms\tremaining: 0us\n",
      "Brier Score: 0.2418853706566385\n"
     ]
    }
   ],
   "source": [
    "# Model is trained here - but the predictions are combinations of *all* possible pairings teams\n",
    "model = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model - uses a brier score?\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, preds)}\")\n",
    "\n",
    "# Generate submission\n",
    "all_teams = np.concatenate((dfs['MTeams']['TeamID'].values, dfs['WTeams']['TeamID'].values))\n",
    "pairings = list(combinations(all_teams, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286903"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required matchup IDs\n",
    "required_ids_df = pd.read_csv(os.path.join(input_folder, \"SampleSubmissionStage2.csv\"))\n",
    "required_ids = set(required_ids_df['ID'])\n",
    "\n",
    "# Submit to kaggle?\n",
    "def create_submission(pairings, season=2025, max_rows=131407):\n",
    "    submission = []\n",
    "    for (team1, team2) in pairings:\n",
    "        matchup_id = f\"{season}_{min(team1, team2)}_{max(team1, team2)}\"\n",
    "        if matchup_id in required_ids:\n",
    "            input_data = pd.DataFrame({'Season': [season], 'WTeamID': [min(team1, team2)], 'LTeamID': [max(team1, team2)]})\n",
    "            pred = model.predict_proba(input_data)[0, 1] if len(input_data) > 0 else 0.5\n",
    "            submission.append([matchup_id, pred])\n",
    "    submission_df = pd.DataFrame(submission, columns=[\"ID\", \"Pred\"])\n",
    "    print(f\"Submission file has {submission_df.shape[0]} rows.\")\n",
    "    return submission_df\n",
    "\n",
    "# Looks like the author locally saves then directly uploads to Kaggle?\n",
    "submission_df = create_submission(pairings)\n",
    "# submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created successfully.\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(submission_df['Pred'], bins=50, kde=True)\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
